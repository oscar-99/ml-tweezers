{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jypyter (Python3) template for training neural network\n",
    "This jupyter notebook provides an example workflow for training a neural \n",
    "network to predict forces on a particle in an optical trap.\n",
    "To use this file, you will need to modify the file for your specific problem/data,\n",
    "see comments bellow.\n",
    "The output will be a trained network, saved in a `.h5` file which can be loaded\n",
    "back into python or into another language (such as Matlab).\n",
    "\n",
    "This template is based on the paper\n",
    "*Machine learning enables fast statistically significant simulations of optically trapped microparticles*\n",
    "(Lenton et al., 2019).\n",
    "This file is released under the GPL-v3 license.\n",
    "If you find this simulation useful, please cite it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "You will need a data set which sufficiently covers the parameter\n",
    "space and provides estimates for the force at each position in the\n",
    "parameter space.\n",
    "For example, this data could include particle position, refractive index\n",
    "and orientation and estimate the force/torque for choices of input parameters.\n",
    "The data set could be experimental data or simulation data.\n",
    "\n",
    "Depending on how your data is saved the following procedure will vary.\n",
    "The two snippets bellow show loading data saved in a Matlab `.mat` file\n",
    "and loading data saved in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from .mat file\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# Load from mat file\n",
    "# you may need to tarnspose depending on how the data is stored\n",
    "positions = np.transpose(loadmat('input.mat')['positions']) # [Nx3]\n",
    "radius = np.transpose(loadmat('input.mat')['radius']) # [Nx1]\n",
    "height = np.transpose(loadmat('input.mat')['height']) # [Nx1]\n",
    "forces = np.transpose(loadmat('input.mat')['forces']) # [Nx3]\n",
    "torque = np.transpose(loadmat('input.mat')['torque']) # [Nx3]\n",
    "\n",
    "# Join input and outputs into single array and store number of features\n",
    "feature_number = 5\n",
    "data = np.concatenate((positions, radius, height, forces, torques), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from .csv file\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "positions = np.empty([0, 3])\n",
    "forces = np.empty([0, 3])\n",
    "\n",
    "with open('input.csv') as fp:\n",
    "    csvfile = csv.reader(fp, delimiter=',')\n",
    "    for line in csvfile:\n",
    "        positions.append([[line[0], line[1], line[2]], axis=0)\n",
    "        forces.append([[line[3], line[4], line[5]]], axis=0)\n",
    "\n",
    "# Join input and outputs into single array and store number of features\n",
    "feature_number = 3\n",
    "data = np.concatenate((positions, forces), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split validation and training data and shuffle\n",
    "We randomly split the data into validation and training data.\n",
    "We use 10% of the data set for validation and reserve the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = data.shape[0]\n",
    "number_val_samples = round(0.1*total_points)\n",
    "number_train_samples = total_points - number_val_samples\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = data[:number_train_samples, :feature_number]\n",
    "train_targets = data[:number_train_samples, feature_number:]\n",
    "\n",
    "val_data = data[number_train_samples:number_train_samples + number_val_samples, :feature_number]\n",
    "val_targets = data[number_train_samples:number_train_samples + number_val_samples, feature_number:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup keras model\n",
    "For most tasks we found that a dense neural network with only a few layers was sufficient to\n",
    "accurately predict the forces on the particle.  The following shows how we could create a\n",
    "network with 3 hidden layers, 1 layer for the inputs and 1 layer for the outputs.\n",
    "For training, we use the [Adam](https://keras.io/optimizers/#adam)\n",
    "optimiser and we use mean squared error for the loss function.\n",
    "We store mean average error and mean average percentage error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "sz = 256;  # Number of nodes per hidden layer\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(sz, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "model.add(layers.Dense(sz, activation='relu'))\n",
    "model.add(layers.Dense(sz, activation='relu'))\n",
    "model.add(layers.Dense(train_targets.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mape'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We train the model using batches of increasing size.\n",
    "The batch size determines how many trial points are evaluated before updating the\n",
    "network weights.\n",
    "For each batch size, we pass over the entire data set `num_epochs` times, storing the\n",
    "error after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = []\n",
    "mape = []\n",
    "val_mae = []\n",
    "val_mape = []\n",
    "\n",
    "batches = [32, 128, 1024, 4096, 16384]\n",
    "num_epochs = 100;   # Fixed number for each batch\n",
    "\n",
    "for epochs, batch_size in zip([num_epochs]*len(batches), batches):\n",
    "    print(\">>> \")\n",
    "    print(\">>> \", batch_size, \" <<<\")\n",
    "    print(\">>> \")\n",
    "\n",
    "    history = model.fit(train_data,\n",
    "                        train_targets,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(val_data, val_targets))\n",
    "\n",
    "    mae.extend(history.history['mean_absolute_error'])\n",
    "    mape.extend(history.history['mean_absolute_percentage_error'])\n",
    "    val_mae.extend(history.history['val_mean_absolute_error'])\n",
    "    val_mape.extend(history.history['val_mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result\n",
    "The output files can be loaded back into python or Matlab for later use.\n",
    "We could also visualise the error inside this script, for example, see `example-3dof-dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_name = \"output.h5\"\n",
    "model.save(save_file_name)\n",
    "\n",
    "import pickle\n",
    "\n",
    "save_file_name_pkl = \"results.pkl\"\n",
    "with open(save_file_name_pkl, 'wb') as f:\n",
    "    pickle.dump([mae, mape, val_mae, val_mape], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
